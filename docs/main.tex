\documentclass{article}
\usepackage{nips10submit_e,times}
%\documentstyle[nips07submit_09,times]{article}
\usepackage[square,numbers]{natbib}
\usepackage{amsmath, epsfig}
\usepackage{amsfonts}
\usepackage{subfigure}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{easybmat}
\usepackage{footmisc}
\renewcommand\algorithmiccomment[1]{// \textit{#1}}
%
\newcommand{\ignore}[1]{}
\newcommand{\comment}[1]{}
\DeclareMathOperator*{\argmax}{arg\,max}

\title{Bayesian Nonparametric Ontology Learning}


\author{
Nicholas Bartlett \hspace{1cm} Matthew Hoffman \hspace{1cm} Frank Wood\\
Columbia University, New York, NY 10027, USA \\
\texttt{\{bartlett, xxx, fwood\}@stat.columbia.edu}
%\texttt{pfau@neurotheory.columbia.edu} 
%\texttt{\{bartlett,fwood\}@stat.columbia.edu} 
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\state}{q}
\newcommand{\symb}{\sigma}
\newcommand{\bmu}{\boldsymbol\mu}
\newcommand{\bphi}{\boldsymbol\phi}
\newcommand{\bpi}{\boldsymbol\pi}
\newcommand{\PY}{\textrm{PY}}
\newcommand{\geom}{\textrm{Geom}}
\newcommand{\unif}{\textrm{Unif}}

\nipsfinalcopy

\begin{document}

\maketitle

\input{abstract}
\input{introduction}

\section{Model Definition}
%% For each word $x_t$, we assume that there is a corresponding binary
%% vector $s_t \in \mathcal{S} \equiv \{0, 1\}^*$.

%% We assume that at each time $t$, a binary vector $z_t$ is drawn
%% according to a process that will be defined below. Conditioned on this
%% vector $z_t$, a corresponding word $w_t$ is drawn according to a
%% hierarchical Pitman-Yor process

At each time step $t$, a Deterministic Infinite Automaton (DIA) $m_t
\in \mathcal{M}$ is chosen according to a Markov process:
\begin{equation}
J_\mu \sim \PY(1, 1, \unif(\mathcal{M}));\quad
m_t \sim J_{m_{t-1}}.
\end{equation}

For each Deterministic Infinite Automaton (DIA) $\mu \in \mathcal{M}
\equiv \{1,\ldots,M\}$, we assume that the parameters $\delta_\mu$
associated with that that DIA are sampled according to the following
process:
\begin{equation}
\theta_{\mu,\epsilon} \sim \PY(c^{\delta}, d^{\delta}_{0},\geom(q)); \quad
\theta_{\mu,\sigma} \sim \PY(c^{\delta}, d^{\delta}_{|\sigma|},
\theta_{\mu,\gamma(\sigma)}); \quad
\delta_{\mu,\zeta,\sigma} \sim \theta_{\mu,\sigma}.
\end{equation}
$\gamma(\sigma)$ is an operator that removes the least significant bit
in the binary vector $\sigma$; for example, $\gamma(011001)=01100$.

Given a sequence of the $H$ binary vectors $s_{t-H:t-1}$ preceding
time $t$, the DIA $m_t$ generates a sequence of states $z_{t,-H:0}$
according to the following deterministic program:
\begin{equation}
z_{t,-H} = 0; \quad
\textrm{for } h \in \{-H+1,\ldots,0\}, z_{t,h} = \delta_{m_t, z_{t,h-1}, s_{t+h}}.
\end{equation}
Shared across all of these DIA is a set of (a countably infinite
number of) emission distributions $\pi_{\zeta}$ over binary
vectors. Each $\pi_{\zeta}$ is defined by a set of $r, l,$ and $s$
variables drawn from a Pitman-Yor process:
\begin{equation}
\begin{split}
\{r_{0,\sigma}, l_{0,\sigma}, u_{0,\sigma}\} &\sim \PY(c_0^\pi, d^\pi_{0}, 
\{\frac{1-\beta}{2}, \frac{1-\beta}{2}, \beta\}); \\
\{r_{\zeta,\sigma}, l_{\zeta,\sigma}, u_{\zeta,\sigma}\} &\sim 
\PY(c_1^\pi, d^\pi_{1}, \{r_{0,\sigma}, l_{0,\sigma}, u_{0,\sigma}\}).
\end{split}
\end{equation}
Given $r, l,$ and $s$, each of these distributions generates binary
state vectors $\sigma$ according to the following process:
\begin{enumerate}
\item Initialize $\sigma$ to the empty vector $\epsilon$.
\item Until the process is stopped:
  \begin{enumerate}
  \item Either append a 1 to $\sigma$ (with probability $r_{z_{t,0}, \sigma}$),
    append a 0 to $\sigma$ (with probability $l_{z_{t,0}, \sigma}$), or
    stop the process (with probability $u_{z_{t,0},\sigma}$).
  \end{enumerate}
\end{enumerate}
%% \begin{equation}
%% \begin{split}
%% s_{t,0} &\sim \{r_{z_{t,0}, \epsilon}, l_{z_{t,0}, \epsilon}, 
%% s_{z_{t,0}, \epsilon}\}; \\
%% s_{t,b} &\sim \{r_{z_{t,0}, s_{t,1:b-1}}, l_{z_{t,0}, s_{t,1:b-1}}, 
%% s_{z_{t,0}, s_{t,1:b-1}}\}
%% \end{split}
%% \end{equation}
We denote the probability of generating a particular binary vector
$\sigma$ under the emission distribution indexed by $\zeta$ as
$\pi_{\zeta}(\sigma)$. Thus, we can write
\begin{equation}
s_t \sim \pi_{z_{t,0}}.
\end{equation}
Conditioned on the latent binary state vector $s_t$ for time $t$, we
draw word $w_t$ according to a hierarchical Pitman-Yor process:
\begin{equation}
\begin{split}
G_{\epsilon} \sim \PY(c^G, d^G_{0},\unif(1,\ldots,W)); \quad
G_z \sim \PY(c^G, d^G_{|z|},G_{\gamma(z)}); \quad
w_t \sim G_{z_t}.
\end{split}
\end{equation}
$W$ is the size of the vocabulary.


%% \input{abstract}

%% \input{introduction}
%% \input{pdfa}

%% \input{model}

%% %\input{inference}


%% %\input{previous_work}

%% \input{results}
%% \input{theory}

%% \input{discussion}

\newpage

%\subsubsection*{Acknowledgments}

%\subsubsection*{References}
\begin{small}
\bibliographystyle{plainnat}
\bibliography{../../columbia/papers/uber} 
%\input{modrefs}
\end{small}
\end{document}
