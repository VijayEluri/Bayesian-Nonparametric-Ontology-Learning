\documentclass{article}
\usepackage{nips10submit_e,times}
%\documentstyle[nips07submit_09,times]{article}
\usepackage[square,numbers]{natbib}
\usepackage{amsmath, epsfig}
\usepackage{amsfonts}
\usepackage{subfigure}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{easybmat}
\usepackage{footmisc}
\renewcommand\algorithmiccomment[1]{// \textit{#1}}
%
\newcommand{\ignore}[1]{}
\newcommand{\comment}[1]{}
\DeclareMathOperator*{\argmax}{arg\,max}

\title{Bayesian Nonparametric Ontology Learning}


\author{
Nicholas Bartlett \hspace{1cm} Matthew Hoffman \hspace{1cm} Frank Wood\\
Columbia University, New York, NY 10027, USA \\
\texttt{\{bartlett, xxx, fwood\}@stat.columbia.edu}
%\texttt{pfau@neurotheory.columbia.edu} 
%\texttt{\{bartlett,fwood\}@stat.columbia.edu} 
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\state}{q}
\newcommand{\symb}{\sigma}
\newcommand{\bmu}{\boldsymbol\mu}
\newcommand{\bphi}{\boldsymbol\phi}
\newcommand{\bpi}{\boldsymbol\pi}
\newcommand{\PY}{\textrm{PY}}
\newcommand{\geom}{\textrm{geom}}

\nipsfinalcopy

\begin{document}

\maketitle

\section{Introduction}

Bayesian nonparametrics rules.

Ontologies rule.

Learning rules.

What could go wrong?

\section{Model Definition}
We assume that at each time $t$, a binary vector $z_t$ is drawn
according to a process that will be defined below. Conditioned on this
vector $z_t$, a corresponding word $w_t$ is drawn according to a
hierarchical Pitman-Yor process

We begin by assuming that the parameters $\delta$ for $M$
Deterministic Infinite Automata (DIA) are sampled according to the
following process:
\begin{equation}
\theta_{m,\epsilon} \sim \PY(d^{\delta}_{0},\geom(q)); \quad
\theta_{m,\sigma_j} \sim \PY(d^{\delta}_{|\sigma_j|},
\theta_{m,\gamma(\sigma_j)}); \quad
\delta_{m,\phi_i,\sigma_j} \sim \theta_{m,\sigma_j}.
\end{equation}
Given a sequence $z_{1:V}$ of binary vectors, the DIA defined by
$\delta_m$ produces the following sequence of states $\phi_{1:V}$:
\begin{equation}
\phi_0 = 0; \phi_i = \delta_{m, \phi_{i-1}, z_i}.
\end{equation}

%%  Each automaton $m$
%% defines a deterministic function that maps from input streams of
%% binary vectors $\sigma$ to an output state $\phi$.


Shared across all of these PDFA is a set of (a countably infinite
number of) emission distributions $\pi_{\phi_i}$ over binary
vectors. Each of these distributions generates binary state vectors
$z$ according to the following process:
\begin{equation}
\begin{split}
\{r_{0,\sigma_j}, l_{0,\sigma_j}, s_{0,\sigma_j}\} &\sim \PY(d^\pi_{0}, 
\{\frac{1-\beta}{2}, \frac{1-\beta}{2}, \beta\}); \\
\{r_{\phi_i,\sigma_j}, l_{\phi_i,\sigma_j}, s_{\phi_i,\sigma_j}\} &\sim \PY(d^\pi_{1}, 
\{r_{0,\sigma_j}, l_{0,\sigma_j}, s_{0,\sigma_j}\}); \\
z_{t,0} &\sim \{r_{\phi_t, \epsilon}, l_{\phi_t, \epsilon}, 
s_{\phi_t, \epsilon}\}; \\
z_{t,b} &\sim \{r_{\phi_t, z_{t,1:b-1}}, l_{\phi_t, z_{t,1:b-1}}, 
s_{\phi_t, z_{t,1:b-1}}\}.
\end{split}
\end{equation}
We denote the probability of generating a binary $z$ vector under
one of these emission distributions as $\pi_{\phi_i}(z)$.

%% Having sampled $\delta$, latent binary state vectors $z_t$ are sampled
%% for each time $t$ according to the following process:
%% \begin{equation}
%% p(\phi_{t,1:V} | z_{t-V:t-1}) \propto
%% \prod_{i=0}^{V-1}\mathbb{I}[\delta_{m_t, \phi_{t, i}, z_{t-V+i}}=\phi_{t,i+1}]
%% \pi_{\phi_{t,i}}(z_{t-V+i}); \quad
%% p(z_t | \phi_{t,V}) = \pi_{\phi_{t, V}}(z_t).
%% \end{equation}
%% In English, we first sample a sequence $\phi_{t, 1:V}$ from the
%% posterior predictive distribution of PDFA $m_t$ conditioned on
%% $z_{t-V:t-1}$, then draw $z_t$ from the distribution $\pi_{\phi_{t,
%%     V}}$ associated with state $\phi_{t, V}$.

Conditioned on the latent binary state vector $z_t$ for time $t$, we
draw word $w_t$ according to a hierarchical Pitman-Yor process:
\begin{equation}
\begin{split}
G_{\epsilon} \sim \PY(d^G_{0},H); \quad
G_z \sim \PY(d^G_{|z|},G_{\gamma(z)}); \quad
w_t \sim G_{z_t}.
\end{split}
\end{equation}


%% \input{abstract}

%% \input{introduction}
%% \input{pdfa}

%% \input{model}

%% %\input{inference}


%% %\input{previous_work}

%% \input{results}
%% \input{theory}

%% \input{discussion}

\newpage

%\subsubsection*{Acknowledgments}

%\subsubsection*{References}
\begin{small}
\bibliographystyle{plainnat}
\bibliography{../../uber} 
%\input{modrefs}
\end{small}
\end{document}
